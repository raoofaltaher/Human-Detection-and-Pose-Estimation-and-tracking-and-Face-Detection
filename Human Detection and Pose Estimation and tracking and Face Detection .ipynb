{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount GOOGLE COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20388,
     "status": "ok",
     "timestamp": 1704983280625,
     "user": {
      "displayName": "RAOOF ALTAHER",
      "userId": "00572918516945130980"
     },
     "user_tz": -180
    },
    "id": "k3Zfe5YJ6F40",
    "outputId": "e4620971-f23a-47e1-a403-39f5a1b45cee"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29MxUiKFLUrM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install mediapipe\n",
    "!pip install ultralytics\n",
    "!pip install opencv-python numpy\n",
    "!pip uninstall torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu112 #Modify CUDA version based on Installed version of CUDA i have the 11.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cpu\n",
      "False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1704998025663,
     "user": {
      "displayName": "RAOOF ALTAHER",
      "userId": "00572918516945130980"
     },
     "user_tz": -180
    },
    "id": "9QFLODVqnFql",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.checks import check_imshow\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import string\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MediaPipe Pose Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1704998027235,
     "user": {
      "displayName": "RAOOF ALTAHER",
      "userId": "00572918516945130980"
     },
     "user_tz": -180
    },
    "id": "HAC8RiyOnF5X",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Pose Detector\n",
    "class poseDetector():\n",
    "    def __init__(self, mode=False, upBody=False, smooth=True, detectionCon=0.8, trackCon=0.8, landmarkThickness=1):\n",
    "        self.mode = mode\n",
    "        self.upBody = upBody\n",
    "        self.smooth = smooth\n",
    "        self.detectionCon = detectionCon\n",
    "        self.trackCon = trackCon\n",
    "        self.landmarkThickness = landmarkThickness  # New parameter for landmark thickness\n",
    "        self.mpDraw = mp.solutions.drawing_utils\n",
    "        self.mpPose = mp.solutions.pose\n",
    "        self.pose = self.mpPose.Pose(static_image_mode=self.mode,\n",
    "                                     model_complexity=1,\n",
    "                                     smooth_landmarks=self.smooth,\n",
    "                                     enable_segmentation=False,\n",
    "                                     smooth_segmentation=False,\n",
    "                                     min_detection_confidence=self.detectionCon,\n",
    "                                     min_tracking_confidence=self.trackCon)\n",
    "\n",
    "    def findPose(self, img, draw=True):\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        self.results = self.pose.process(imgRGB)\n",
    "        if self.results.pose_landmarks and draw:\n",
    "            self.mpDraw.draw_landmarks(img, self.results.pose_landmarks, self.mpPose.POSE_CONNECTIONS)\n",
    "        return img\n",
    "\n",
    "    def findPosition(self, img, draw=True):\n",
    "        lmList = []\n",
    "        if self.results.pose_landmarks:\n",
    "            for id, lm in enumerate(self.results.pose_landmarks.landmark):\n",
    "                h, w, c = img.shape\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                lmList.append([id, cx, cy])\n",
    "                if draw:\n",
    "                    # Use the specified thickness for landmarks\n",
    "                    cv2.circle(img, (cx, cy), 1, (255, 99, 51), self.landmarkThickness)\n",
    "        return lmList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MediaPipe Face Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize MediaPipe face Detector\n",
    "class FaceDetector():\n",
    "    \n",
    "    def __init__(self, minDetectionCon = 0.7):\n",
    "        \n",
    "        self.minDetectionCon = minDetectionCon      \n",
    "        self.mpFaceDetection = mp.solutions.face_detection\n",
    "        self.mpDraw = mp.solutions.drawing_utils\n",
    "        self.faceDetection = self.mpFaceDetection.FaceDetection(self.minDetectionCon)\n",
    "\n",
    "        \n",
    "    def fancyDraw(self, img, bbox, color=(255, 0, 255), thickness=2):\n",
    "        x, y, w, h = bbox\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), color, thickness)\n",
    "        # Add more fancy drawing code here if needed\n",
    "        return img\n",
    "\n",
    "\n",
    "    def findFaces(self, img, draw = True):\n",
    "        \n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        self.results = self.faceDetection.process(imgRGB)\n",
    "        # print(self.results)\n",
    "        \n",
    "        bboxs = []\n",
    "        if self.results.detections:\n",
    "            for id, detection in enumerate(self.results.detections):\n",
    "                # mpDraw.draw_detection(img, detection)            \n",
    "                # print(id, detection)\n",
    "                # print(detection.score)\n",
    "                # print(detection.location_data.relative_bounding_box)\n",
    "                \n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih , iw ,ic = img.shape\n",
    "                bbox = int(bboxC.xmin * iw), int(bboxC.ymin * ih),\\\n",
    "                    int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "                \n",
    "                bboxs.append([id, bbox, detection.score])\n",
    "                if draw:\n",
    "                    cv2.rectangle(img, bbox, (255, 0, 255), 1)\n",
    "        return img, bboxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize ultralytics - YOLOv8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize YOLOv8 \n",
    "model = YOLO(\"YOLOv8 - models/yolov8x.pt\")  # Use 0 for webcam input\n",
    "names = model.model.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialize - MediaPipe Detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Detectors\n",
    "detector = poseDetector(landmarkThickness=1)\n",
    "face_detector = FaceDetector(minDetectionCon = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 1011,
     "status": "ok",
     "timestamp": 1704998029982,
     "user": {
      "displayName": "RAOOF ALTAHER",
      "userId": "00572918516945130980"
     },
     "user_tz": -180
    },
    "id": "hMo6td7Cy4zs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Video processing\n",
    "video_path = \"input_dir/6.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "assert cap.isOpened(), \"Error reading video file\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Frame processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)  # Use the input video's frame rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a unique random filename for the output video"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate a unique random filename for the output video\n",
    "def generate_random_filename(extension=\".mp4\"):\n",
    "    \"\"\"Generate a random filename.\"\"\"\n",
    "    letters = string.ascii_lowercase\n",
    "    random_string = ''.join(random.choice(letters) for i in range(10))\n",
    "    return f\"{random_string}{extension}\"\n",
    "\n",
    "random_filename = generate_random_filename()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a filename based on the current date and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20240112_170753.mp4'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_timestamp_filename(extension=\".mp4\"):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    return f\"{timestamp}{extension}\"\n",
    "\n",
    "# Generate a timestamp-based filename for the output video\n",
    "timestamp_filename = generate_timestamp_filename()\n",
    "\n",
    "timestamp_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Video File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using 'mp4v' codec for MP4 file format\n",
    "# out = cv2.VideoWriter(\"output.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), frame_rate, (frame_width, frame_height))\n",
    "\n",
    "\n",
    "# Using 'mp4v' codec for MP4 file format with the random filename\n",
    "# out = cv2.VideoWriter(random_filename, cv2.VideoWriter_fourcc(*'mp4v'), frame_rate, (frame_width, frame_height))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using 'mp4v' codec for MP4 file format with the timestamp filename\n",
    "out = cv2.VideoWriter(timestamp_filename, cv2.VideoWriter_fourcc(*'mp4v'), frame_rate, (frame_width, frame_height)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "track_history = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize flags\n",
    "draw_bounding_box = True\n",
    "draw_face_bounding_box = False\n",
    "draw_landmarks = False\n",
    "draw_tracking = True\n",
    "draw_nose_to_corner_line = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#specific landmark to the bottom-left corner if the flag is set\n",
    "\n",
    "# 0 is the nose\n",
    "\n",
    "Landmark_Bottom_Left_Corner_Line = 0  # Set your desired landmark index here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Real-Time video frame process or no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize flags\n",
    "display_video = False  # Set to False to process in background without display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break  # Exit loop if there are no more frames to read      \n",
    "    \n",
    "        # Apply YOLO object detection on the frame to find humans\n",
    "        results = model.track(frame, persist=True, verbose=False)\n",
    "        boxes = results[0].boxes.xyxy.cpu()  # Extract bounding boxes\n",
    "\n",
    "        # Process each detected object\n",
    "        if results[0].boxes.id is not None:\n",
    "            clss = results[0].boxes.cls.cpu().tolist()  # Class IDs of detections\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()  # Tracking IDs\n",
    "\n",
    "            # Initialize annotator for drawing bounding boxes if the flag is set\n",
    "            annotator = Annotator(frame, line_width=2) if draw_bounding_box else None\n",
    "\n",
    "            for box, cls, track_id in zip(boxes, clss, track_ids):\n",
    "                if names[int(cls)] != \"person\":\n",
    "                    continue\n",
    "\n",
    "                if draw_bounding_box:\n",
    "                    annotator.box_label(box, color=colors(int(cls), True), label=names[int(cls)])\n",
    "\n",
    "                x1, y1, x2, y2 = [int(b) for b in box[:4]]\n",
    "                human_img = frame[y1:y2, x1:x2]\n",
    "\n",
    "                # Perform face detection within the YOLO bounding box if the flag is set\n",
    "                if draw_face_bounding_box:\n",
    "                    face_img_rgb = cv2.cvtColor(human_img, cv2.COLOR_BGR2RGB)\n",
    "                    human_img, face_detections = face_detector.findFaces(face_img_rgb, draw=False)\n",
    "                    if face_detections:\n",
    "                        for _, bbox, _ in face_detections:\n",
    "                            bx, by, bw, bh = bbox\n",
    "                            cv2.rectangle(frame, (x1 + bx, y1 + by), (x1 + bx + bw, y1 + by + bh), (0, 255, 0), 2)\n",
    "\n",
    "                # Perform pose estimation on the cropped human image\n",
    "                human_img = detector.findPose(human_img, draw=False)\n",
    "                lmList = detector.findPosition(human_img, draw=draw_landmarks)\n",
    "\n",
    "                # Tracking logic: Update tracking history and draw tracking lines if the flag is set\n",
    "                if draw_tracking:\n",
    "                    center = (int((x1 + x2) / 2), int((y1 + y2) / 2))\n",
    "                    track_history[track_id].append(center)\n",
    "                    if len(track_history[track_id]) > 30:\n",
    "                        track_history[track_id].pop(0)\n",
    "\n",
    "                    if len(track_history[track_id]) > 1:\n",
    "                        points = np.array(track_history[track_id], dtype=np.int32).reshape((-1, 1, 2))\n",
    "                        cv2.polylines(frame, [points], isClosed=False, color=colors(int(cls), True), thickness=1)\n",
    "\n",
    "                # Draw a line from a specific landmark to the bottom-left corner if the flag is set\n",
    "                if draw_nose_to_corner_line:\n",
    "                    for lm in lmList:\n",
    "                        if lm[0] == Landmark_Bottom_Left_Corner_Line:  # Use the variable here\n",
    "                            chosen_landmark_point = (lm[1] + x1, lm[2] + y1)\n",
    "                            bottom_left_corner = (0, frame_height)\n",
    "                            cv2.line(frame, chosen_landmark_point, bottom_left_corner, (0, 255, 255), 1)\n",
    "\n",
    "        # Write the processed frame to output\n",
    "        if draw_bounding_box:\n",
    "            out.write(annotator.result())\n",
    "        else:\n",
    "            out.write(frame)\n",
    "\n",
    "        # Display the processed frame if the flag is set (optional, based on your requirement)\n",
    "        if display_video:\n",
    "            cv2.imshow(\"Processed Frame\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "finally:\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()  # Ensure this is called to finalize the video file\n",
    "    cv2.destroyAllWindows()\n",
    "    track_history.clear()\n",
    "    print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process all videos in a directory and export an output video for each one into an output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize YOLOv8 and MediaPipe Detectors\n",
    "# model = YOLO(\"YOLOv8 - models/yolov8x.pt\")\n",
    "detector = poseDetector(landmarkThickness=1)\n",
    "face_detector = FaceDetector(minDetectionCon=0.9)\n",
    "names = model.model.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_directory = \"input_dir\"\n",
    "output_directory = \"output_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "Path(output_directory).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List all video files in the input directory\n",
    "video_files = glob.glob(os.path.join(input_directory, \"*.mp4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize flags\n",
    "draw_bounding_box = True\n",
    "draw_face_bounding_box = False\n",
    "draw_landmarks = False\n",
    "draw_tracking = True\n",
    "draw_nose_to_corner_line = True\n",
    "draw_global_face_detection = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#specific landmark to the bottom-left corner if the flag is set\n",
    "\n",
    "# 0 is the nose\n",
    "\n",
    "Landmark_Bottom_Left_Corner_Line = 0  # Set your desired landmark index here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# Process each video file\n",
    "for video_path in video_files:\n",
    "    # Reinitialize the YOLOv8 model at the start of each video\n",
    "    model = YOLO(\"YOLOv8 - models/yolov8x.pt\")  # Adjust model path as needed\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        continue\n",
    "\n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Generate a unique output filename\n",
    "    output_filename = os.path.join(output_directory, os.path.basename(video_path))\n",
    "\n",
    "    # Initialize VideoWriter for the current video dimensions\n",
    "    out = cv2.VideoWriter(output_filename, cv2.VideoWriter_fourcc(*'mp4v'), frame_rate, (frame_width, frame_height))\n",
    "\n",
    "    track_history = defaultdict(list)  # Initialize tracking history for each video\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break  # Exit loop if there are no more frames to read\n",
    "                \n",
    "        if draw_global_face_detection:\n",
    "            # Perform global face detection on the entire frame\n",
    "            frame, bboxs = face_detector.findFaces(frame, draw=True)\n",
    "        \n",
    "    \n",
    "        # Apply YOLO object detection on the frame to find humans\n",
    "        results = model.track(frame, persist=True, verbose=False)\n",
    "        boxes = results[0].boxes.xyxy.cpu()  # Extract bounding boxes\n",
    "\n",
    "        # Process each detected object\n",
    "        if results[0].boxes.id is not None:\n",
    "            clss = results[0].boxes.cls.cpu().tolist()  # Class IDs of detections\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()  # Tracking IDs\n",
    "\n",
    "            # Initialize annotator for drawing bounding boxes if the flag is set\n",
    "            annotator = Annotator(frame, line_width=2) if draw_bounding_box else None\n",
    "\n",
    "            for box, cls, track_id in zip(boxes, clss, track_ids):\n",
    "                if names[int(cls)] != \"person\":\n",
    "                    continue\n",
    "\n",
    "                if draw_bounding_box:\n",
    "                    annotator.box_label(box, color=colors(int(cls), True), label=names[int(cls)])\n",
    "\n",
    "                x1, y1, x2, y2 = [int(b) for b in box[:4]]\n",
    "                human_img = frame[y1:y2, x1:x2]\n",
    "\n",
    "                # Perform face detection within the YOLO bounding box if the flag is set\n",
    "                if draw_face_bounding_box:\n",
    "                    face_img_rgb = cv2.cvtColor(human_img, cv2.COLOR_BGR2RGB)\n",
    "                    human_img, face_detections = face_detector.findFaces(face_img_rgb, draw=False)\n",
    "                    if face_detections:\n",
    "                        for _, bbox, _ in face_detections:\n",
    "                            bx, by, bw, bh = bbox\n",
    "                            cv2.rectangle(frame, (x1 + bx, y1 + by), (x1 + bx + bw, y1 + by + bh), (0, 255, 0), 2)\n",
    "\n",
    "                # Perform pose estimation on the cropped human image\n",
    "                human_img = detector.findPose(human_img, draw=False)\n",
    "                lmList = detector.findPosition(human_img, draw=draw_landmarks)\n",
    "\n",
    "                # Tracking logic: Update tracking history and draw tracking lines if the flag is set\n",
    "                if draw_tracking:\n",
    "                    center = (int((x1 + x2) / 2), int((y1 + y2) / 2))\n",
    "                    track_history[track_id].append(center)\n",
    "                    if len(track_history[track_id]) > 30:\n",
    "                        track_history[track_id].pop(0)\n",
    "\n",
    "                    if len(track_history[track_id]) > 1:\n",
    "                        points = np.array(track_history[track_id], dtype=np.int32).reshape((-1, 1, 2))\n",
    "                        cv2.polylines(frame, [points], isClosed=False, color=colors(int(cls), True), thickness=1)\n",
    "\n",
    "                # Draw a line from a specific landmark to the bottom-left corner if the flag is set\n",
    "                if draw_nose_to_corner_line:\n",
    "                    for lm in lmList:\n",
    "                        if lm[0] == Landmark_Bottom_Left_Corner_Line:  # Use the variable here\n",
    "                            chosen_landmark_point = (lm[1] + x1, lm[2] + y1)\n",
    "                            bottom_left_corner = (0, frame_height)\n",
    "                            cv2.line(frame, chosen_landmark_point, bottom_left_corner, (255, 0, 255), 1)\n",
    "\n",
    "       # Write the processed frame to output\n",
    "        if draw_bounding_box:\n",
    "            out.write(annotator.result())\n",
    "        else:\n",
    "            out.write(frame)\n",
    "\n",
    "    # After processing each video, clear the tracking history\n",
    "    track_history.clear()\n",
    "\n",
    "    # Release resources for the current video\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMAocviQtbQC8iV5WsTqRCi",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "yolov8-env",
   "language": "python",
   "name": "yolov8-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
